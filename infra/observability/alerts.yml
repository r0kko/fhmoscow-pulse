groups:
  - name: app-slo
    rules:
      # SLO: availability 99% (1% error budget). Burn-rate style alerts.
      - record: slo:availability:error_ratio:5m
        expr: |
          sum(rate(http_requests_total{route!="/metrics",status=~"5.."}[5m]))
          /
          sum(rate(http_requests_total{route!="/metrics"}[5m]))
      - record: slo:availability:error_ratio:30m
        expr: |
          sum(rate(http_requests_total{route!="/metrics",status=~"5.."}[30m]))
          /
          sum(rate(http_requests_total{route!="/metrics"}[30m]))
      - record: slo:availability:error_ratio:1h
        expr: |
          sum(rate(http_requests_total{route!="/metrics",status=~"5.."}[1h]))
          /
          sum(rate(http_requests_total{route!="/metrics"}[1h]))
      - record: slo:availability:error_ratio:6h
        expr: |
          sum(rate(http_requests_total{route!="/metrics",status=~"5.."}[6h]))
          /
          sum(rate(http_requests_total{route!="/metrics"}[6h]))

      # Burn rate = error_ratio / (1 - SLO). SLO = 0.99 â†’ error budget 0.01
      - record: slo:availability:burnrate:5m
        expr: slo:availability:error_ratio:5m / 0.01
      - record: slo:availability:burnrate:30m
        expr: slo:availability:error_ratio:30m / 0.01
      - record: slo:availability:burnrate:1h
        expr: slo:availability:error_ratio:1h / 0.01
      - record: slo:availability:burnrate:6h
        expr: slo:availability:error_ratio:6h / 0.01

      # Fast burn (page): 5m and 1h burn > 14.4 (Google SRE rec.)
      - alert: SLOErrorBudgetBurnFast
        expr: slo:availability:burnrate:5m > 14.4 and slo:availability:burnrate:1h > 14.4
        for: 2m
        labels:
          severity: page
        annotations:
          summary: "High error-budget burn (fast)"
          description: |
            Error budget burning fast (>14.4x) over 5m and 1h.
            Current burn rate (5m window) shown as alert value.

      # Slow burn (ticket): 30m and 6h windows
      - alert: SLOErrorBudgetBurnSlow
        expr: slo:availability:burnrate:30m > 6 and slo:availability:burnrate:6h > 1
        for: 10m
        labels:
          severity: ticket
        annotations:
          summary: "Error-budget burn (slow)"
          description: |
            Error budget burning over 30m and 6h windows.
            Check dashboard for detailed ratios and routes.

      # Simple 5xx spike
      - alert: Http5xxSpike
        expr: |
          sum(rate(http_requests_total{route!="/metrics",status=~"5.."}[5m]))
          >
          0
          and
          sum(rate(http_requests_total{route!="/metrics"}[5m])) > 0
        for: 5m
        labels:
          severity: page
        annotations:
          summary: "HTTP 5xx rate detected"
          description: |
            5xx requests present over the last 5 minutes.

      # CSRF rejection ratio > 1% of POST requests over 10m
      - alert: CsrfRejectionHigh
        expr: |
          sum(rate(csrf_rejected_total[10m]))
          /
          clamp_min(sum(rate(http_requests_total{method="POST"}[10m])), 1)
          > 0.01
        for: 10m
        labels:
          severity: ticket
        annotations:
          summary: "CSRF rejections elevated (>1%)"
          description: |
            Increase in CSRF rejections over 10 minutes. Investigate security dashboard.

      # EBADCSRFTOKEN surge (absolute rate)
      - alert: CsrfTokenMismatchSpike
        expr: sum(rate(http_error_code_total{code="EBADCSRFTOKEN"}[5m])) > 1
        for: 5m
        labels:
          severity: ticket
        annotations:
          summary: "EBADCSRFTOKEN spike"
          description: "More than 1 EBADCSRFTOKEN per second over 5 minutes"

  - name: runtime-health
    rules:
      # Event loop lag (average > 250ms for 5m)
      - alert: NodeEventLoopLagHigh
        expr: avg_over_time(nodejs_eventloop_lag_seconds[5m]) > 0.25
        for: 5m
        labels:
          severity: ticket
        annotations:
          summary: "High Node.js event loop lag"
          description: "Average event loop lag > 250ms"

      # Process RSS > 700MB for 10m (adjust to your container limits)
      - alert: AppProcessMemoryHigh
        expr: avg_over_time(process_resident_memory_bytes[10m]) > 7e8
        for: 10m
        labels:
          severity: ticket
        annotations:
          summary: "App memory usage is high"
          description: "process_resident_memory_bytes > 700MB for 10m"

      # Container memory near limit (requires cAdvisor)
      - alert: ContainerMemoryPressure
        expr: |
          (container_memory_working_set_bytes{container_label_com_docker_compose_service="app"}
            / ignoring (id)
           container_spec_memory_limit_bytes{container_label_com_docker_compose_service="app"})
          > 0.9
        for: 5m
        labels:
          severity: page
        annotations:
          summary: "Container memory > 90% of limit"
          description: "App container working set exceeds 90% of memory limit"

      # Container CPU usage near quota (if set)
      - alert: ContainerCpuHigh
        expr: |
          sum by (container_label_com_docker_compose_service) (rate(container_cpu_usage_seconds_total{container_label_com_docker_compose_service="app"}[5m]))
          /
          clamp_min(
            sum by (container_label_com_docker_compose_service) (container_spec_cpu_quota{container_label_com_docker_compose_service="app"}
              / container_spec_cpu_period{container_label_com_docker_compose_service="app"}),
            1)
          > 0.8
        for: 10m
        labels:
          severity: ticket
        annotations:
          summary: "High CPU usage vs quota"
          description: "App CPU usage > 80% of CPU limit for 10m"

  - name: infra-core
    rules:
      # Any target down
      - alert: PrometheusJobDown
        expr: up == 0
        for: 5m
        labels:
          severity: ticket
        annotations:
          summary: "Prometheus target down"
          description: "Instance {{ $labels.instance }} of job {{ $labels.job }} is down"

      # Postgres exporter reports DB down
      - alert: PostgresDown
        expr: pg_up == 0
        for: 2m
        labels:
          severity: page
        annotations:
          summary: "PostgreSQL is down"
          description: "postgres-exporter reports pg_up=0"

      # Redis exporter reports instance down
      - alert: RedisDown
        expr: redis_up == 0
        for: 2m
        labels:
          severity: page
        annotations:
          summary: "Redis is down"
          description: "redis_exporter reports redis_up=0"

      # Node filesystem usage > 90%
      - alert: HostDiskSpaceLow
        expr: |
          (
            node_filesystem_avail_bytes{fstype!~"tmpfs|overlay|aufs|squashfs"}
            /
            node_filesystem_size_bytes{fstype!~"tmpfs|overlay|aufs|squashfs"}
          ) < 0.10
        for: 15m
        labels:
          severity: ticket
        annotations:
          summary: "Low disk space (<10%)"
          description: "Filesystem {{ $labels.mountpoint }} low free space"

      # Node filesystem inodes usage > 90%
      - alert: HostDiskInodesLow
        expr: |
          (
            node_filesystem_files_free{fstype!~"tmpfs|overlay|aufs|squashfs"}
            /
            node_filesystem_files{fstype!~"tmpfs|overlay|aufs|squashfs"}
          ) < 0.10
        for: 15m
        labels:
          severity: ticket
        annotations:
          summary: "Low disk inodes (<10%)"
          description: "Filesystem {{ $labels.mountpoint }} low free inodes"

      # App readiness degraded
      - alert: AppNotReady
        expr: app_ready == 0
        for: 5m
        labels:
          severity: ticket
        annotations:
          summary: "App readiness is false"
          description: "app_ready=0 for over 5 minutes"

      # Synthetics probe failing
      - alert: BlackboxProbeFail
        expr: probe_success == 0
        for: 5m
        labels:
          severity: ticket
        annotations:
          summary: "Blackbox probe failure"
          description: "Synthetics health probe failing"

      # Spike in invalid logins
      - alert: LoginFailureSpike
        expr: increase(auth_login_attempts_total{result="invalid"}[5m]) > 20
        for: 5m
        labels:
          severity: ticket
        annotations:
          summary: "Spike of failed logins"
          description: ">20 invalid login attempts in 5m window"

      # Rate limiting surge (global limiter)
      - alert: RateLimitedSurge
        expr: increase(rate_limited_total{name="global"}[5m]) > 100
        for: 5m
        labels:
          severity: ticket
        annotations:
          summary: "Clients frequently rate-limited"
          description: ">100 requests blocked by global rate limiter in 5m"

      # Refresh token invalid ratio > 5% (auth_refresh_total invalid vs total)
      - alert: RefreshInvalidHigh
        expr: |
          sum(increase(auth_refresh_total{result="invalid"}[10m]))
          /
          clamp_min(sum(increase(auth_refresh_total[10m])), 1)
          > 0.05
        for: 10m
        labels:
          severity: ticket
        annotations:
          summary: "High invalid refresh ratio"
          description: ">5% invalid refresh attempts over 10m"

  - name: email-delivery
    rules:
      - alert: EmailQueueBacklogHigh
        expr: max(email_queue_depth{bucket="ready"}) > 100
        for: 10m
        labels:
          severity: ticket
        annotations:
          summary: "Email ready queue backlog high"
          description: |
            Ready queue depth exceeded 100 for 10 minutes.
            Investigate stuck workers or external SMTP slowness.

      - alert: EmailDeadLetterNonEmpty
        expr: max(email_queue_depth{bucket="dead_letter"}) > 0
        for: 5m
        labels:
          severity: ticket
        annotations:
          summary: "Emails stuck in dead-letter queue"
          description: |
            Dead-letter queue contains undelivered messages. Check DLQ dashboard and logs.

      - alert: EmailDeliveryFailures
        expr: sum(rate(email_queue_failure_total[5m])) > 0
        for: 5m
        labels:
          severity: page
        annotations:
          summary: "Email delivery failures detected"
          description: |
            Email queue failures > 0 over 5 minutes. See Grafana email dashboard and SMTP logs.

      - alert: EmailLatencyHigh
        expr: histogram_quantile(0.95, sum by (le) (rate(email_delivery_duration_seconds_bucket{status="ok"}[5m]))) > 15
        for: 10m
        labels:
          severity: ticket
        annotations:
          summary: "Email delivery latency p95 > 15s"
          description: |
            Email delivery p95 latency above 15 seconds for 10 minutes. Check SMTP provider health.
